{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction-Computational-Graphs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Computational Graphs with PyTorch\n",
        "\n",
        "by [Shreyas Kale](https://www.linkedin.com/in/shreyaskale11/)\n",
        "\n",
        "\n",
        "In this notebook we provide a short introduction and overview of computational graphs using PyTorch.\n",
        "\n",
        "Inspired by Olah's article [\"Calculus on Computational Graphs: Backpropagation\"](https://colah.github.io/posts/2015-08-Backprop/)."
      ],
      "metadata": {
        "id": "_MbzfbWoqAaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Computational Graphs?"
      ],
      "metadata": {
        "id": "IGzBSo7H6xKu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YuD6zdWZp7DP"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the inputs like this:"
      ],
      "metadata": {
        "id": "b7EKlMrouClt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([3.], requires_grad=True)\n",
        "b = torch.tensor([1.], requires_grad=True)"
      ],
      "metadata": {
        "id": "OZ2pB2A3uEQZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = a + b\n",
        "d = b + 1\n",
        "e = c * d\n",
        "\n",
        "# grads populated for non-leaf nodes\n",
        "c.retain_grad()\n",
        "d.retain_grad()\n",
        "e.retain_grad()"
      ],
      "metadata": {
        "id": "XwXomBUxu1Ib"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t-uhE6vvH2j",
        "outputId": "2cd9c886-2d40-476b-c87c-ae68f6c70391"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([8.], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Derivatives on Computational Graphs\n",
        "\n",
        "Using the concept of computational graphs we are now interested in evaluating the **partial derivatives** of the edges of the graph. This will help in gathering the gradients of the graph. Remember that gradients are what we use to train the neural network and those calculations can be taken care of by the automatic differentation engine. \n",
        "\n",
        "The intuition is: we want to know, for example, if $a$ directly affects $c$, how does it affect it. In other words, if we change $a$ a little, how does $c$ change. This is referred to as the partial derivative of $c$ with respect to $a$.\n",
        "\n",
        "You can work this by hand, but the easy way to do this with PyTorch is by calling `.backward()` on $e$ and let the engine figure out the values. The `.backward()` signals the autograd engine to calculate the gradients and store them in the respective tensors’ `.grad` attribute.\n",
        "\n",
        "Let's do that now:"
      ],
      "metadata": {
        "id": "tjX3LCRmw22a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "e.backward()"
      ],
      "metadata": {
        "id": "Nc6lnO5yy1Cq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s say we are interested in the derivative of $e$ with respect to $a$, how do we obtain this? In other words, we are looking for $\\frac{\\partial e}{\\partial a}$."
      ],
      "metadata": {
        "id": "hxbtx6OCy3I8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using PyTorch, we can do this by calling `a.grad`:"
      ],
      "metadata": {
        "id": "NvQcK9LTzD34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NWnWDg4zHDn",
        "outputId": "56bb6607-f627-4cea-a01c-44f8056ff71c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to understand the intuition behind this. Olah puts it best:\n",
        "\n",
        ">Let’s consider how $e$ is affected by $a$. If we change $a$ at a speed of 1, $c$ also changes at a speed of $1$. In turn, $c$ changing at a speed of $1$ causes $e$ to change at a speed of $2$. So $e$ changes at a rate of $1*2$ with respect to $a$.\n"
      ],
      "metadata": {
        "id": "c05nEObzzbPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check that this holds, let look at another example. How about caluclating the derivative of $e$ with respect to $b$, i.e., $\\frac{\\partial e}{\\partial b}$?\n",
        "\n",
        "We can get that through `b.grad`:"
      ],
      "metadata": {
        "id": "9uZE-Gl12cnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q11abV90d6i",
        "outputId": "ac84b12b-0b22-4474-f8e3-11619c56ea73"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are all the gradients collected, including non-leaf nodes:"
      ],
      "metadata": {
        "id": "sbJvhj5m13Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.grad, b.grad, c.grad, d.grad, e.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrUxwsrd3-f-",
        "outputId": "ac41af14-f3aa-4711-f879-84b5dce3f728"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.]) tensor([6.]) tensor([2.]) tensor([4.]) tensor([1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the computational graph above to verify that everything is correct. This is the power of computational graphs and how they are used by automatic differentation engines. It's also a very useful concept to understand when developing neural networks architectures and their correctness."
      ],
      "metadata": {
        "id": "HftIH5Mx4Pdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Steps\n",
        "\n",
        "In this notebook, I've provided a simple and intuitive explanation to the concept of computational graphs using PyTorch. I highly recommend to go through [Olah's article](https://colah.github.io/posts/2015-08-Backprop/) for more on the topic.\n",
        "\n",
        "In the next tutorial, I will be applying the concept of computational graphs to more advanced operations you typically see in a neural network. In fact, if you are interested in this, and you are feeling comfortable with the topic now, you can check out these two PyTorch tutorials:\n",
        "\n",
        "- [A gentle introduction to `torch.autograd`](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
        "- [Automatic differentation with `torch.autograd`](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)\n",
        "\n",
        "And here are some other useful references used to put this article together:\n",
        "\n",
        "- [Hacker's guide to Neural Networks\n",
        "](http://karpathy.github.io/neuralnets/)\n",
        "- [Backpropagation calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8&ab_channel=3Blue1Brown)\n",
        "\n"
      ],
      "metadata": {
        "id": "DxyJDoMOs1gu"
      }
    }
  ]
}